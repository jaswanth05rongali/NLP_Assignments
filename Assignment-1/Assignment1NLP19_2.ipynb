{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 on Natural Language Processing\n",
    "\n",
    "## Date : 26th July, 2019\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Ishani Mondal, Debanjana Kar, Sukannya Purkayastha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python library, nltk. Please find the installation details below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of NLTK and Anaconda:\n",
    "\n",
    "To ensure we are all on the same page, the coding environment will be in `python3`. We suggest downloading \n",
    "Anaconda3 and creating a separate environment to do this assignment. <br> \n",
    "\n",
    "\n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. <br>\n",
    "The steps to install NLTK is available on the link: <br>\n",
    "\n",
    "`sudo pip3 install nltk` <br>\n",
    "`python3` <br>\n",
    "`nltk.download()` <br>\n",
    "\n",
    "To install gensim, use the following command: (detailed tutorial) <br>\n",
    "`conda install -c conda-forge gensim` <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Note : For the purpose of your convenience, we are also providing you with a demo hands-on ipython notebook explaining the basics of language modelling using nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks\n",
    "\n",
    "Use the corpus given. Ignore the .concept files and use the .txt files for each disease abstract.\n",
    "\n",
    "### Task A: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "**Tokenize** the corpus into sentences and words (for each of the pos and neg class). **Print the number of sentences and words.** <br>\n",
    "**Perform case-folding** on the corpus. <br>\n",
    "**Remove the stopwords** from the corpus and print the count of the rest of the non stop-words occurring in the corpus.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the code for Task A\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os,glob\n",
    "\n",
    "#tokenizing the corpus\n",
    "os.chdir('../NCBI_Data')                #give the path to folder containing the corpus\n",
    "data=[]\n",
    "corpus=''\n",
    "for file_name in glob.glob(\"*.txt\"):\n",
    "    file = open(file_name)\n",
    "    corpus += file.read()\n",
    "corpus = corpus.casefold()           #case-folding\n",
    "sentences = sent_tokenize(corpus)\n",
    "words = word_tokenize(corpus)\n",
    "print(len(sentences))\n",
    "print(len(words))\n",
    "\n",
    "#Removing the stop-words\n",
    "stop_words = set(stopwords.words('english'))                \n",
    "non_stop_words = []\n",
    "for word in corpus_words:\n",
    "    if not word in stop_words:\n",
    "        non_stop_words.append(word)\n",
    "print(len(non_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the code for Task B\n",
    "\n",
    "from nltk.util import ngrams\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "#n=2\n",
    "unigrams.extend(words)\n",
    "bigrams.extend(ngrams(words,2))\n",
    "trigrams.extend(ngrams(words,3))\n",
    "fourgrams.extend(ngrams(words,4))\n",
    "##similar for trigrams and fourgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in unigrams if p not in stop_words]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "fdist = dict(sorted(fdist.items(), key=lambda x: x[1], reverse=True))\n",
    "print('unigrams')\n",
    "print(list(fdist.items())[:10])                        #The top 10 occuring unigrams along with their no. of occurences\n",
    "print(list(fdist.keys())[:10])\n",
    "\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "\n",
    "#bigrams\n",
    "bi_processed = [item for item in bigrams if (item[0] not in stop_words) and (item[1] not in stop_words)]\n",
    "bi_fdist = nltk.FreqDist(bi_processed)\n",
    "bi_fdist = dict(sorted(bi_fdist.items(), key=lambda x: x[1], reverse=True))\n",
    "print('bigrams')\n",
    "print(list(bi_fdist.items())[:10])                  #The top 10 bigrams(interms of no. of occurences) along with their no. of occurences\n",
    "\n",
    "#trigrams\n",
    "tri_processed = [item for item in trigrams if (item[0] not in stop_words) and (item[1] not in stop_words) and (item[2] not in stop_words)]\n",
    "tri_fdist = nltk.FreqDist(tri_processed)\n",
    "tri_fdist = dict(sorted(tri_fdist.items(), key=lambda x: x[1], reverse=True))\n",
    "print('trigrams')\n",
    "print(list(tri_fdist.items())[:10])                 #The top 10 trigrams(interms of no. of occurences) along with their no. of occurences\n",
    "\n",
    "#fourgrams\n",
    "four_processed = [item for item in fourgrams if (item[0] not in stop_words) and (item[1] not in stop_words) and (item[2] not in stop_words) and (item[3] not in stop_words)]\n",
    "four_fdist = nltk.FreqDist(four_processed)\n",
    "four_fdist = dict(sorted(four_fdist.items(), key=lambda x: x[1], reverse=True))\n",
    "print('fourgrams')\n",
    "print(list(four_fdist.items())[:10])            #The top 10 fourgrams(interms of no. of occurences) along with their no. of occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "# Probability(unigram) = count(unigram)/Number of unique unigrams + Total number of unigrams\n",
    "\n",
    "#unigram\n",
    "uni_prob = {}\n",
    "for key,value in fdist.items():\n",
    "    uni_prob[key] = ((value+1)/(len(fdist)+sum(fdist.values())))\n",
    "\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "\n",
    "#bigram\n",
    "bi_prob = {}\n",
    "for key,value in bi_fdist.items():\n",
    "    bi_prob[key] = ((value+1)/(len(bi_fdist)+sum(bi_fdist.values())))\n",
    "\n",
    "#trigram\n",
    "tri_prob = {}\n",
    "for key,value in tri_fdist.items():\n",
    "    tri_prob[key] = ((value+1)/(len(tri_fdist)+sum(tri_fdist.values())))\n",
    "\n",
    "\n",
    "#fourgram\n",
    "four_prob = {}\n",
    "for key,value in four_fdist.items():\n",
    "    four_prob[key] = ((value+1)/(len(four_fdist)+sum(four_fdist.values())))\n",
    "\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "print('unigrams')\n",
    "print(list(uni_prob.items())[:10])\n",
    "print('bigrams')\n",
    "print(list(bi_prob.items())[:10])\n",
    "print('trigrams')\n",
    "print(list(tri_prob.items())[:10])\n",
    "print('fourgrams')\n",
    "print(list(four_prob.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word given the previous n(=2, 3, 4)-grams** for the sentences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'A new tumor suppressor gene, PTEN/MMAC1, was isolated recently'\n",
    "str2 = 'The average age of disease'\n",
    "\n",
    "string = str1       #change string to str2 to check for str2\n",
    "\n",
    "#bigram\n",
    "print('bigram')\n",
    "word = string.split()[-1]\n",
    "i = 0\n",
    "for key,value in bi_prob.items():\n",
    "    if key[0] == word:\n",
    "        print('The predicted next word is: {}'.format(key[1]))  #Since already the probabilities are sorted in descending order, the first occurence of the bigram containing the first word as 'recently' will be the most probable case. \n",
    "        i = i+1\n",
    "    if i == 2:\n",
    "        break\n",
    "        \n",
    "#trigram\n",
    "print('trigram')\n",
    "words = []\n",
    "words = string.split()\n",
    "i = 0\n",
    "for key,value in tri_prob.items():\n",
    "    if key[0] == words[-2] and key[1] == words[-1]:\n",
    "        print('The predicted next word is: {}'.format(key[2]))\n",
    "        i = i+1\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "#fourgram\n",
    "print('fourgram')\n",
    "words = []\n",
    "words = string.split()\n",
    "i = 0\n",
    "for key,value in four_prob.items():\n",
    "    if key[0] == words[-3] and key[1] == words[-2] and key[2] == words[-1]:\n",
    "        print('The predicted next word is: {}'.format(key[3]))\n",
    "        i = i+1\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For str1, you are to predict the next  2 possible word sequences using your trained smoothed models. The answers can be as below:()\n",
    "    1) 'A new tumor suppressor gene, PTEN/MMAC1, was isolated recently' *genuinely*\n",
    "    2)  'A new tumor suppressor gene, PTEN/MMAC1, was isolated recently' *yesterday*\n",
    "For str2, you are to predict the next 2 possible word sequences using your trained smoothed models such as:\n",
    "    (1) 'The average age of disease', *hinders*\n",
    "    (2) 'The average age of disease', *past*\n",
    "The above answers are not solutions but just examples to explain the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C: In this task, you are to perform the following tasks:\n",
    "\n",
    "1. **Train word vectors** on the given corpus. In order to train the word vectors on your corpus, using the gensim module (https://radimrehurek.com/gensim/models/word2vec.html) with pre-trained Google word embeddings (GoogleNews-vectors) . For multi-word disease mentions, concatenate each word with a ‘_’.  <br>\n",
    "\n",
    "2. **Construct a t-SNE plot** of the trained word vectors of the disease mentions.\n",
    "\n",
    "3. **Repeat experiment 1. and 2.** using the following hyper-parameter settings:\n",
    "Use window size = 5, 10.<br>\n",
    "Use embedding dimension = 50, 100, 200.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the code for Task C\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_word_vec(embed_size,w_size):\n",
    "    global disease\n",
    "    disease = []\n",
    "    sentences = []\n",
    "    text = []\n",
    "#     global transformed_words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for filename in os.listdir('NCBI_Data/'):\n",
    "        if '.txt' in filename:\n",
    "            conc_names = []\n",
    "            disease_names = []\n",
    "            with open('NCBI_Data/'+filename[:-4]+'.concept') as f:\n",
    "                for line in f:\n",
    "                    if 'Disease' in line:\n",
    "                        line = line.split('||')[-2]\n",
    "                        disease_names.append(line)\n",
    "                        conc_names.append(line.replace(' ','_'))\n",
    "                        disease.append(conc_names[-1])\n",
    "#                         transformed_words[disease_names[-1]] = conc_names[-1]\n",
    "            with open('NCBI_Data/'+filename) as f:\n",
    "                corpus = f.read()\n",
    "                for i in range(len(conc_names)):\n",
    "                    corpus = corpus.replace(disease_names[i],conc_names[i])\n",
    "                corpus = corpus.lower()\n",
    "                sentences += sent_tokenize(corpus)\n",
    "                words = [w for w in word_tokenize(corpus) if w not in stop_words]\n",
    "                text.append(words)\n",
    "                \n",
    "    #Using the pre-trained glove word vectors\n",
    "    glove_input_file = 'glove.6B.'+str(embed_size)+'d.txt'\n",
    "    word2vec_output_file = 'glove.6B.'+str(embed_size)+'d.txt.word2vec'\n",
    "    glove2word2vec(glove_input_file,word2vec_output_file)\n",
    "    \n",
    "    model = Word2Vec(size=embed_size, window=w_size, min_count=1)    \n",
    "    model.build_vocab(text)\n",
    "    model.intersect_word2vec_format(word2vec_output_file, lockf=1.0)\n",
    "    model.train(text, epochs = 150, total_examples=model.corpus_count)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model5_50 = generate_word_vec(50,5)\n",
    "model10_50 = generate_word_vec(50,10)\n",
    "model5_100 = generate_word_vec(100,5)\n",
    "model10_100 = generate_word_vec(100,10)\n",
    "model5_200 = generate_word_vec(200,5)\n",
    "model10_200 = generate_word_vec(200,10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(model):\n",
    "    # Get similar words and their WordVectors.\n",
    "    g = np.empty((0, model.wv.vector_size))\n",
    "    for d in disease:\n",
    "        if d in model.wv.vocab:\n",
    "            g = np.append(g, np.array([model.wv[d]]), axis=0)\n",
    "            \n",
    "        tsne = TSNE(n_components=2, random_state=1)\n",
    "    \n",
    "    tsne = tsne.fit_transform(g)\n",
    "    X = tsne[:, 0]\n",
    "    Y = tsne[:, 1]\n",
    "    plt.scatter(X, Y)\n",
    "    for label, x, y in zip(disease, X, Y):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "#     plt.axes(ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax)),option='on')\n",
    "    plt.xlim(X.min() - .5, X.max() + .5)\n",
    "    plt.ylim(Y.min() - .5, Y.max() + .5)\n",
    "    plt.show()\n",
    "\n",
    "plot(model5_50)\n",
    "plot(model5_100)\n",
    "plot(model5_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D: Predict the next word using neural language modelling\n",
    "\n",
    "Using LSTM Language modelling, you are expected to **train your own word vectors and predict the next word, given the context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for Task D\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "## Prepare the corpus from the .txt files and store it in a string variable i.e data_str. It should contain the \n",
    "## sentences splitted by \"\\n\".\n",
    "import os, glob\n",
    "\n",
    "\n",
    "files = os.listdir('../NCBI_Data')\n",
    "data=[]\n",
    "for file in files:\n",
    "    if file.endswith('.txt'):\n",
    "        f = open('../NCBI_Data/'+file,'r')\n",
    "        content = f.read()\n",
    "#         print(f1)\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if(line!=\"\"):\n",
    "                data.append(line)\n",
    "        f.close()\n",
    "\n",
    "#print(data)\n",
    "data_str=\"\\n\".join(data)\n",
    "# print(data_str)\n",
    "\n",
    "\n",
    "# generate the sequence \n",
    "\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length,truncating='pre')\n",
    "    yhat = model.predict_classes(encoded, verbose=0)\n",
    "    out_word = tokenizer.sequences_to_texts(list(yhat))\n",
    "    in_text+= ' '+ out_word\n",
    "    return in_text\n",
    " \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data_str])\n",
    "\n",
    "# Write the code for encoding text to sequences here and store in encoded\n",
    "encoded = tokenizer.texts_to_sequences([data_str])\n",
    "\n",
    "# retrieve vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# encode 2 words -> 1 word\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "    sequence = encoded[i-2:i+1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'A new tumor suppressor gene, PTEN/MMAC1, was isolated recently', 1))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'The average age of disease', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
