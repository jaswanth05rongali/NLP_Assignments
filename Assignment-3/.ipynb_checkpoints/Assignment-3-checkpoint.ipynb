{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pickle, tempfile\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import svm\n",
    "from nltk.parse import DependencyGraph, DependencyEvaluator\n",
    "from nltk.parse.transitionparser import TransitionParser, Configuration, Transition\n",
    "from os import remove\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from numpy import array\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing data\n",
    "def load_data(filename):\n",
    "    tg = []\n",
    "    with open(filename,'r') as f:\n",
    "        con= f.read().split('\\n\\n')\n",
    "#         print(\"Number of sentences :\",len(con),end='\\n\\n')\n",
    "        for s in con:\n",
    "            t = s.split('\\n')\n",
    "            s = [i for i in t if not i.startswith('#')]\n",
    "            tg.append(DependencyGraph(s))\n",
    "    return tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_eng = load_data('Corpus/eng.train.conllu')\n",
    "test_graph_eng = load_data('Corpus/eng.test.conllu')\n",
    "train_graph_sp = load_data('Corpus/spanish.train.conllu')\n",
    "test_graph_sp = load_data('Corpus/spanish.test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modifiedConfiguration(Configuration):\n",
    "    def extract_features(self, include_features):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                if include_features['word']:\n",
    "                    result.append('STK_0_FORM_' + token['word'])\n",
    "                else:\n",
    "                    result.append('STK_0_FORM_' + '#')\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                if include_features['lemma']:\n",
    "                    result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "                else:\n",
    "                    result.append('STK_0_LEMMA_' + '#')\n",
    "            if self._check_informative(token['tag']):\n",
    "                if include_features['tag']:\n",
    "                    result.append('STK_0_POS_' + token['tag'])\n",
    "                else:\n",
    "                    result.append('STK_0_POS_' + '#')\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                if include_features['feats']:\n",
    "                    for feat in feats:\n",
    "                        result.append('STK_0_FEATS_' + feat)\n",
    "                else:\n",
    "                    result.append('STK_0_FEATS_' + '#')\n",
    "                    \n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    if include_features['tag']:\n",
    "                        result.append('STK_1_POS_' + token['tag'])\n",
    "                    else:\n",
    "                        result.append('STK_1_POS_' + '#')\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                if include_features['word']:\n",
    "                    result.append('BUF_0_FORM_' + token['word'])\n",
    "                else:\n",
    "                    result.append('BUF_0_FORM_' + '#')\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                if include_features['lemma']:\n",
    "                    result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "                else:\n",
    "                    result.append('BUF_0_LEMMA_' + '#')\n",
    "            if self._check_informative(token['tag']):\n",
    "                if include_features['tag']:\n",
    "                    result.append('BUF_0_POS_' + token['tag'])\n",
    "                else:\n",
    "                    result.append('BUF_0_POS_' + '#')\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                if include_features['feats']:\n",
    "                    for feat in feats:\n",
    "                        result.append('BUF_0_FEATS_' + feat)\n",
    "                else:\n",
    "                    result.append('BUF_0_FEATS_' + '#')\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    if include_features['word']:\n",
    "                        result.append('BUF_1_FORM_' + token['word'])\n",
    "                    else:\n",
    "                        result.append('BUF_1_FORM_' + '#')\n",
    "                if self._check_informative(token['tag']):\n",
    "                    if include_features['tag']:\n",
    "                        result.append('BUF_1_POS_' + token['tag'])\n",
    "                    else:\n",
    "                        result.append('BUF_1_POS_' + '#')\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    if include_features['tag']:\n",
    "                        result.append('BUF_2_POS_' + token['tag'])\n",
    "                    else:\n",
    "                        result.append('BUF_2_POS_' + '#')\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    if include_features['tag']:\n",
    "                        result.append('BUF_3_POS_' + token['tag'])\n",
    "                    else:\n",
    "                        result.append('BUF_3_POS_' + '#')\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modifiedTransitionParser(TransitionParser):\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file, include_features):\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = modifiedConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features(include_features)\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "    \n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file, include_features):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = modifiedConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features(include_features)\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "    \n",
    "    def train(self, depgraphs, modelfile, include_features, modeltype, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file, include_features)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file, include_features)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            if modeltype=='mlp':\n",
    "                model = MLPClassifier()\n",
    "            elif modeltype == 'logistic_reg':\n",
    "                model = LogisticRegression()\n",
    "            elif modeltype == 'svm':\n",
    "                model = svm.SVC(\n",
    "                    kernel='poly',\n",
    "                    degree=2,\n",
    "                    coef0=0,\n",
    "                    gamma=0.2,\n",
    "                    C=0.5,\n",
    "                    verbose=verbose,\n",
    "                    probability=True)\n",
    "                \n",
    "            model.fit(x_train, y_train.ravel())\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "            \n",
    "    def parse(self, depgraphs, modelFile, include_features):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = modifiedConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features(include_features)\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(transition_type,de_model,model,string):\n",
    "    \n",
    "    if transition_type == 'arc-standard':\n",
    "        tr = 'Arc Standard'\n",
    "    else:\n",
    "        tr = 'Arc Eager'\n",
    "    \n",
    "    if model == 'svm':\n",
    "        m = 'SVC'\n",
    "    elif model == 'logistic_reg':\n",
    "        m = 'Logistic Regression'\n",
    "    else:\n",
    "        m = 'MLP'\n",
    "    \n",
    "    print(tr+' Transition | '+m+' | '+string)\n",
    "    print('Labeled Attachment Score   : {:0.4f}'.format(de_model.eval()[0]))\n",
    "    print('Unlabeled Attachment Score : {:0.4f}'.format(de_model.eval()[1]))\n",
    "    \n",
    "\n",
    "def transition_type(transition_type,train_dg,test_dg,model):\n",
    "    \n",
    "    if model == 'svm':\n",
    "        print('----- SVM -----')\n",
    "    elif model == 'logistic_reg':\n",
    "        print('----- Logistic Regression -----')\n",
    "    else:\n",
    "        print('----- MLP -----')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('With all features --- ')\n",
    "    include_features = {'word': True,'lemma': True,'tag': True,'feats':True}\n",
    "    parser_std = modifiedTransitionParser(transition_type)\n",
    "    parser_std.train(train_dg,'arcstd1.model',include_features,model,verbose=False)\n",
    "\n",
    "    result = parser_std.parse(test_dg ,'arcstd1.model', include_features)\n",
    "    de_model = DependencyEvaluator(result, test_dg)\n",
    "    print_results(transition_type,de_model,model,'All features')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Without morphological features ---')\n",
    "    include_features = {'word': True,'lemma': True,'tag': True,'feats':False}\n",
    "    parser_std = modifiedTransitionParser(transition_type)\n",
    "    parser_std.train(train_dg,'arcstd2.model',include_features,model,verbose=False)\n",
    "\n",
    "    result = parser_std.parse(test_dg ,'arcstd2.model', include_features)\n",
    "    de_model = DependencyEvaluator(result, test_dg)\n",
    "    print_results(transition_type,de_model,model,'w/o morphological features')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Without word features ---')\n",
    "    include_features = {'word': False,'lemma': True,'tag': True,'feats':True}\n",
    "    parser_std = modifiedTransitionParser(transition_type)\n",
    "    parser_std.train(train_dg,'arcstd3.model',include_features,model,verbose=False)\n",
    "\n",
    "    result = parser_std.parse(test_dg ,'arcstd3.model', include_features)\n",
    "    de_model = DependencyEvaluator(result, test_dg)\n",
    "    print_results(transition_type,de_model,model,'w/o word features')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Without lemma features ---')\n",
    "    include_features = {'word': True,'lemma': False,'tag': True,'feats':True}\n",
    "    parser_std = modifiedTransitionParser(transition_type)\n",
    "    parser_std.train(train_dg,'arcstd4.model',include_features,model,verbose=False)\n",
    "\n",
    "    result = parser_std.parse(test_dg ,'arcstd4.model', include_features)\n",
    "    de_model = DependencyEvaluator(result, test_dg)\n",
    "    print_results(transition_type,de_model,model,'w/o lemma features')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Without POS features')\n",
    "    include_features = {'word': True,'lemma': True,'tag': False,'feats':True}\n",
    "    parser_std = modifiedTransitionParser(transition_type)\n",
    "    parser_std.train(train_dg,'arcstd5.model',include_features,model,verbose=False)\n",
    "\n",
    "    result = parser_std.parse(test_dg ,'arcstd5.model', include_features)\n",
    "    de_model = DependencyEvaluator(result, test_dg)\n",
    "    print_results(transition_type,de_model,model,'w/o PoS features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc - Standard Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('             ARC-STANDARD Transition          ')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- eng.train.conllu && Test file --- eng.test.conllu')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_eng,'svm')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_eng,'logistic_reg')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_eng,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- eng.train.conllu && Test file --- spanish.test.conllu')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_sp,'svm')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_sp,'logistic_reg')\n",
    "transition_type('arc-standard',train_graph_eng,test_graph_sp,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- spanish.train.conllu && Test file --- spanish.test.conllu')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_sp,'svm')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_sp,'logistic_reg')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_sp,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- spanish.train.conllu && Test file --- eng.test.conllu')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_eng,'svm')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_eng,'logistic_reg')\n",
    "transition_type('arc-standard',train_graph_sp,test_graph_eng,'mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc - Eager Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('             ARC-EAGER Transition          ')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- eng.train.conllu && Test file --- eng.test.conllu')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_eng,'svm')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_eng,'logistic_reg')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_eng,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- eng.train.conllu && Test file --- spanish.test.conllu')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_sp,'svm')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_sp,'logistic_reg')\n",
    "transition_type('arc-eager',train_graph_eng,test_graph_sp,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- spanish.train.conllu && Test file --- spanish.test.conllu')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_sp,'svm')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_sp,'logistic_reg')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_sp,'mlp')\n",
    "\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Train file --- spanish.train.conllu && Test file --- eng.test.conllu')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_eng,'svm')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_eng,'logistic_reg')\n",
    "transition_type('arc-eager',train_graph_sp,test_graph_eng,'mlp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
